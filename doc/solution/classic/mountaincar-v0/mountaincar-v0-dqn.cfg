{'_fake_gpus': False,
 'action_space': None,
 'actions_in_input_normalized': False,
 'adam_epsilon': 1e-08,
 'batch_mode': 'truncate_episodes',
 'before_learn_on_batch': None,
 'buffer_size': 50000,
 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,
 'clip_actions': False,
 'clip_rewards': None,
 'collect_metrics_timeout': 180,
 'compress_observations': False,
 'create_env_on_driver': False,
 'custom_eval_function': None,
 'custom_resources_per_worker': {},
 'double_q': True,
 'dueling': True,
 'eager_tracing': False,
 'env': 'MountainCar-v0',
 'env_config': {},
 'env_task_fn': None,
 'evaluation_config': {'explore': False,
                       'record_env': '/media/hdd-stuff/home/bob/Stuff/python/rllib-exercises/tmp/run/MountainCar-v0/video/0x88e30e1d',
                       'render_env': True},
 'evaluation_interval': 9999999999,
 'evaluation_num_episodes': 3,
 'evaluation_num_workers': 1,
 'evaluation_parallel_to_training': False,
 'exploration_config': {'epsilon_timesteps': 10000,
                        'final_epsilon': 0.02,
                        'initial_epsilon': 1.0,
                        'type': 'EpsilonGreedy'},
 'explore': True,
 'extra_python_environs_for_driver': {},
 'extra_python_environs_for_worker': {},
 'fake_sampler': False,
 'final_prioritized_replay_beta': 0.4,
 'framework': 'tf',
 'gamma': 0.99,
 'grad_clip': 40,
 'hiddens': [256],
 'horizon': None,
 'ignore_worker_failures': False,
 'in_evaluation': False,
 'input': 'sampler',
 'input_config': {},
 'input_evaluation': ['is', 'wis'],
 'learning_starts': 1000,
 'local_tf_session_args': {'inter_op_parallelism_threads': 8,
                           'intra_op_parallelism_threads': 8},
 'log_level': 'WARN',
 'log_sys_usage': True,
 'logger_config': None,
 'lr': 0.0005,
 'lr_schedule': None,
 'metrics_smoothing_episodes': 100,
 'min_iter_time_s': 1,
 'model': {'fcnet_hiddens': [16, 4]},
 'monitor': -1,
 'multiagent': {'count_steps_by': 'env_steps',
                'observation_fn': None,
                'policies': {},
                'policies_to_train': None,
                'policy_map_cache': None,
                'policy_map_capacity': 100,
                'policy_mapping_fn': None,
                'replay_mode': 'independent'},
 'n_step': 1,
 'no_done_at_end': False,
 'noisy': False,
 'normalize_actions': True,
 'num_atoms': 1,
 'num_cpus_for_driver': 1,
 'num_cpus_per_worker': 0,
 'num_envs_per_worker': 1,
 'num_gpus': 0,
 'num_gpus_per_worker': 0,
 'num_workers': 1,
 'observation_filter': 'NoFilter',
 'observation_space': None,
 'optimizer': {},
 'output': None,
 'output_compress_columns': ['obs', 'new_obs'],
 'output_max_file_size': 67108864,
 'placement_strategy': 'PACK',
 'postprocess_inputs': False,
 'preprocessor_pref': 'deepmind',
 'prioritized_replay': True,
 'prioritized_replay_alpha': 0.6,
 'prioritized_replay_beta': 0.4,
 'prioritized_replay_beta_annealing_timesteps': 20000,
 'prioritized_replay_eps': 1e-06,
 'record_env': False,
 'remote_env_batch_wait_ms': 0,
 'remote_worker_envs': False,
 'render_env': False,
 'replay_sequence_length': 1,
 'rollout_fragment_length': 4,
 'sample_async': False,
 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,
 'seed': 2296581661,
 'shuffle_buffer_size': 0,
 'sigma0': 0.5,
 'simple_optimizer': -1,
 'soft_horizon': False,
 'synchronize_filters': True,
 'target_network_update_freq': 500,
 'tf_session_args': {'allow_soft_placement': True,
                     'device_count': {'CPU': 1},
                     'gpu_options': {'allow_growth': True},
                     'inter_op_parallelism_threads': 2,
                     'intra_op_parallelism_threads': 2,
                     'log_device_placement': False},
 'timesteps_per_iteration': 1000,
 'train_batch_size': 32,
 'training_intensity': None,
 'v_max': 10.0,
 'v_min': -10.0,
 'worker_side_prioritization': False}
